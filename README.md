# Document-classification-for-Arabic-language

Introduction:
In recent years, the growing availability of text data in various languages has created significant opportunities for advancements in natural language processing (NLP) applications such as sentiment analysis, machine translation, and information retrieval. Arabic, one of the most widely spoken languages in the world, presents unique challenges for NLP due to its rich morphology, complex syntax, and regional dialects. To address these challenges, high-quality datasets are essential, and the KALIMAT dataset plays a crucial role in advancing Arabic NLP.

  1.  KALIMAT Dataset:
     
       The KALIMAT dataset is one of the largest publicly available Arabic text corpora. It was created to assist in the development of Arabic NLP tools and has been widely adopted by researchers and developers
       alike. The dataset contains millions of Arabic words gathered from various sources, including online news platforms, blogs, and social media networks.

       KALIMAT is an Arabic natural language resource that consists of:

        1.	20,291 Arabic articles collected from the Omani newspaper Alwatan by (Abbas et al. 2011).
        2.	20,291 Extractive Single-document system summaries.
        3.	2,057 Extractive Multi-document system summaries.
        4.	20,291 Named Entity Recognised articles.
        5.	20,291 Part of Speech Tagged articles.
        6.	20,291 Morphologically Analyse articles.

        The data collection articles fall into six categories:
          culture, economy, local-news, international-news, religion, and sports.
          But in the code, I have used only economy, religion and sports.

        The process of creating KALIMAT was applied to the entire data collection (20,291 articles).

  2. Preprocessing:

       	Apply preprocessing techniques specific to Arabic text, such as tokenization, stemming, and normalization.
       	Remove unnecessary elements such as punctuation, special characters, and stopwords.
       	create relevant features such as term frequency, TF-IDF vectors, or word embeddings.
     
  3. Model Selection and Training:

       	Select suitable machine learning or deep learning models (e.g., SVM, Naïve Bayes, KNN).
       	Train and validate the models on the preprocessed data, and tune hyperparameters to optimize performance.

  4. Evaluation:

       	Evaluate the model using appropriate metrics such as accuracy, precision, recall, and F1-score.

       
     

       




        
          

      

